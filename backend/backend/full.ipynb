{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/homebrew/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "import threading\n",
    "import serial\n",
    "import time\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# websocket\n",
    "from flask import render_template\n",
    "from flask_socketio import SocketIO, emit\n",
    "import base64\n",
    "import json\n",
    "# import pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask App\n",
    "app = Flask(__name__)\n",
    "socketio = SocketIO(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/homebrew/lib/python3.10/site-packages')\n",
    "# check sys path for /opt/homebrew/lib/python3.10/site-packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages is in sys.path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Path to check\n",
    "path_to_check = \"/opt/homebrew/lib/python3.10/site-packages\"\n",
    "\n",
    "# Check if the path is in sys.path\n",
    "if path_to_check in sys.path:\n",
    "    print(f\"{path_to_check} is in sys.path\")\n",
    "else:\n",
    "    print(f\"{path_to_check} is not in sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of sign language readings\n",
    "sign_language_dataset = {'A': [3916, 3939, 4015, 4004, 3950, 4036],\n",
    " 'B': [4034, 4100, 3966, 4034, 4067, 3976],\n",
    " 'C': [3966, 3950, 3950, 3899, 3956, 4046],\n",
    " 'D': [4035, 4030, 4075, 3956, 3965, 3956],\n",
    " 'E': [4059, 3975, 4140, 3967, 4012, 4053],\n",
    " 'F': [3976, 3957, 4039, 4041, 4007, 3934],\n",
    " 'G': [3956, 3952, 3934, 4031, 4052, 3957],\n",
    " 'H': [4049, 4024, 4093, 4016, 3882, 4002],\n",
    " 'I': [3952, 3998, 3989, 4026, 4008, 3937],\n",
    " 'J': [3994, 4042, 3939, 4015, 4034, 3960],\n",
    " 'K': [4082, 3998, 3996, 3998, 3850, 3961],\n",
    " 'L': [4032, 3983, 3970, 3981, 4034, 4067],\n",
    " 'M': [4029, 3991, 4004, 3908, 4020, 3954],\n",
    " 'N': [4058, 4002, 3983, 3985, 4012, 3986],\n",
    " 'O': [4039, 4100, 3963, 4020, 4024, 3979],\n",
    " 'P': [4051, 4037, 3950, 4004, 4024, 3922],\n",
    " 'Q': [3937, 4055, 3977, 4010, 4022, 4005],\n",
    " 'R': [3966, 4057, 4024, 4024, 3981, 3935],\n",
    " 'S': [3944, 3983, 3847, 4003, 3997, 4105],\n",
    " 'T': [4005, 3954, 3963, 4071, 4028, 3981],\n",
    " 'U': [4011, 4056, 3957, 4001, 4063, 3926],\n",
    " 'V': [4005, 4121, 4006, 4027, 4132, 3951],\n",
    " 'W': [3994, 3959, 3983, 3974, 3958, 3926],\n",
    " 'X': [3986, 3939, 3963, 3975, 3944, 4008],\n",
    " 'Y': [3951, 3955, 3949, 4071, 3942, 4083],\n",
    " 'Z': [3961, 3943, 4052, 4006, 4010, 4001]}\n",
    "\n",
    "\n",
    "client = OpenAI(api_key='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')\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    return dot(A, B) / (norm(A) * norm(B))\n",
    "\n",
    "def compare_readings(sensorArray, sign_language_dataset):\n",
    "    best = 0\n",
    "    best_key = ''\n",
    "    for key, value in sign_language_dataset.items():\n",
    "        similarity = cosine_similarity(sensorArray, value)\n",
    "        if similarity > best:\n",
    "            best = similarity\n",
    "            best_key = key\n",
    "    return best_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/coolg/treehacks2024/aidactyl/backend/backend', '/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/coolg/Library/Python/3.11/lib/python/site-packages', '/opt/homebrew/lib/python3.11/site-packages', '/opt/homebrew/lib/python3.10/site-packages', '/opt/homebrew/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "# sys.path.append('/opt/homebrew/lib/python3.10/site-packages')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post request to openAI\n",
    "def callOpenAI(words):\n",
    "    try:\n",
    "        # Join the words into a single string as prompt for the API\n",
    "        prompt = ' '.join(words)\n",
    "        \n",
    "        # Call the OpenAI API with the prompt\n",
    "        completion = client.chat.completions.create(\n",
    "\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"I will be passing you glosses of American Sign Language (ASL) words and phrases. Please provide the spoken English translation.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the text from the response\n",
    "        generated_text = completion.choices[0].message.content\n",
    "        print(\"Spoken text:\", generated_text)\n",
    "        spokenText.append(generated_text)\n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARDUINO READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# RECIEVING FROM ARDUINO\n",
    "# Replace '/dev/ttyUSB0' with your serial port and adjust baudrate\n",
    "ser = serial.Serial('/dev/cu.usbmodem2101', 9600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameWordCounter = 0\n",
    "spokenText = []\n",
    "words = []\n",
    "reading = False\n",
    "\n",
    "def startHandCapture(ser):\n",
    "    global words, sameWordCounter, reading\n",
    "    while reading:\n",
    "        data = ser.readline()\n",
    "        if data:\n",
    "            sensorCapture = data.decode('utf-8').strip()\n",
    "            print(sensorCapture)\n",
    "            # convert sensorCapture to an array of integers based on seperating the string by a delimiter\n",
    "            sensorArray = sensorCapture.split(',')\n",
    "            # cast every element in the array to an integer\n",
    "            sensorArray = [int(i) for i in sensorArray]\n",
    "            print(sensorArray)\n",
    "            try:\n",
    "                word = compare_readings(sensorArray, sign_language_dataset)  # Process the data\n",
    "            except:\n",
    "                # go to next loop\n",
    "                continue\n",
    "            # check if equal to the last word received\n",
    "            if len(words) == 0 or words[-1] != word:\n",
    "                words.append(word)  # Append the word to the array\n",
    "            # else:\n",
    "            #     sameWordCounter += 1\n",
    "\n",
    "            # if sameWordCounter == 3:\n",
    "            #     callOpenAI(words)\n",
    "            #     sameWordCounter = 0\n",
    "            #     words = []\n",
    "            print(\"word: \", word)\n",
    "            print(\"words: \", words)\n",
    "            time.sleep(2)  # Wait for 2 seconds before processing the next piece of data\n",
    "\n",
    "def stopHandCapture():\n",
    "    global reading\n",
    "    reading = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUME EMOTION VISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeImage(json):\n",
    "    image_data = json['image']\n",
    "    image_bytes = base64.b64decode(image_data)\n",
    "    with open(\"image.jpg\", \"wb\") as image_file:\n",
    "        image_file.write(image_bytes)\n",
    "    print(\"Image stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hume import HumeBatchClient\n",
    "from hume.models.config import FaceConfig\n",
    "\n",
    "def callHumeAPI():\n",
    "\n",
    "\n",
    "    client = HumeBatchClient(\"a6nLyAAG6WerOIyqNiaUfdljGEBxPSNpflWtScb0O521e7we\")\n",
    "    filepaths = [\n",
    "        \"./assets/image.jpg\" \n",
    "    ]\n",
    "    config = FaceConfig()\n",
    "    job = client.submit_job(None, [config], files=filepaths)\n",
    "\n",
    "    print(job)\n",
    "    print(\"Running...\")\n",
    "\n",
    "    details = job.await_complete()\n",
    "\n",
    "\n",
    "    # job.download_predictions(\"predictions.json\")\n",
    "    results = job.get_predictions()\n",
    "    # print(type(results))\n",
    "    print(results)\n",
    "    first = results[0]\n",
    "    emotions = first['results']['predictions'][0]['models']['face']['grouped_predictions'][0]['predictions'][0]['emotions']\n",
    "    sorted_audio_emotions = sorted(emotions, key=lambda x: x['score'], reverse=True)\n",
    "    print(\"sorted emotions: \", sorted_audio_emotions)\n",
    "    print(\"top emotion: \", sorted_audio_emotions[0])\n",
    "    return sorted_audio_emotions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callHumeAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBSOCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEB SOCKET\n",
    "@socketio.on('connect')\n",
    "def handle_connect():\n",
    "    print('Client connected')\n",
    "\n",
    "@socketio.on('disconnect')\n",
    "def handle_disconnect():\n",
    "    print('Client disconnected')\n",
    "\n",
    "# JSON format: {'type': type, 'data': data}\n",
    "\n",
    "\n",
    "@socketio.on('image')\n",
    "def handleImage(json):\n",
    "    storeImage(json)\n",
    "    emotion = callHumeAPI()\n",
    "    global streaming_active, reading\n",
    "\n",
    "# @socketio.on('speech')\n",
    "# def handleSpeech(json):\n",
    "    # probably on frontend\n",
    "    \n",
    "\n",
    "\n",
    "@socketio.on('signRequest')\n",
    "def handleSignRequest(json):\n",
    "    if json == \"start\" and not streaming_active:\n",
    "        startHandCapture()\n",
    "        reading = True\n",
    "        # socketio.read_from_port(ser)\n",
    "        emit(\"Started transcription\")\n",
    "    \n",
    "    elif json == \"stop\" and streaming_active:\n",
    "        stopHandCapture()\n",
    "        emit('words', words)\n",
    "        words = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker: Own obsession was more the boy band One Direction or 1 d,\n",
      "speaker: and Harry Styles who sang with the group.\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Example filename: main.py\n",
    "import os\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "import threading\n",
    "\n",
    "from deepgram import (\n",
    "    DeepgramClient,\n",
    "    LiveTranscriptionEvents,\n",
    "    LiveOptions,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# URL for the realtime streaming audio you would like to transcribe\n",
    "URL = \"http://stream.live.vc.bbcmedia.co.uk/bbc_world_service\"\n",
    "\n",
    "API_KEY = os.getenv(\"DG_API_KEY\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # STEP 1: Create a Deepgram client using the API key\n",
    "        deepgram = DeepgramClient(API_KEY)\n",
    "\n",
    "        # STEP 2: Create a websocket connection to Deepgram\n",
    "        dg_connection = deepgram.listen.live.v(\"1\")\n",
    "\n",
    "        # STEP 3: Define the event handlers for the connection\n",
    "        def on_message(self, result, **kwargs):\n",
    "            sentence = result.channel.alternatives[0].transcript\n",
    "            if len(sentence) == 0:\n",
    "                return\n",
    "            print(f\"speaker: {sentence}\")\n",
    "\n",
    "        def on_metadata(self, metadata, **kwargs):\n",
    "            print(f\"\\n\\n{metadata}\\n\\n\")\n",
    "\n",
    "        def on_error(self, error, **kwargs):\n",
    "            print(f\"\\n\\n{error}\\n\\n\")\n",
    "\n",
    "        # STEP 4: Register the event handlers\n",
    "        dg_connection.on(LiveTranscriptionEvents.Transcript, on_message)\n",
    "        dg_connection.on(LiveTranscriptionEvents.Metadata, on_metadata)\n",
    "        dg_connection.on(LiveTranscriptionEvents.Error, on_error)\n",
    "\n",
    "        # STEP 5: Configure Deepgram options for live transcription\n",
    "        options = LiveOptions(\n",
    "            model=\"nova-2\", \n",
    "            language=\"en-US\", \n",
    "            smart_format=True,\n",
    "            )\n",
    "        \n",
    "        # STEP 6: Start the connection\n",
    "        dg_connection.start(options)\n",
    "\n",
    "        # STEP 7: Create a lock and a flag for thread synchronization\n",
    "        lock_exit = threading.Lock()\n",
    "        exit = False\n",
    "\n",
    "        # STEP 8: Define a thread that streams the audio and sends it to Deepgram\n",
    "        def myThread():\n",
    "            # p = pyaudio.PyAudio()\n",
    "            # stream = p.open(format=pyaudio.paInt16,\n",
    "            #             channels=1,\n",
    "            #             rate=16000,\n",
    "            #             input=True,\n",
    "            #             frames_per_buffer=1024)\n",
    "        \n",
    "\n",
    "\n",
    "            with httpx.stream(\"GET\", URL) as r:\n",
    "                for data in r.iter_bytes():\n",
    "                    lock_exit.acquire()\n",
    "                    if exit:\n",
    "                        break\n",
    "                    lock_exit.release()\n",
    "\n",
    "                    dg_connection.send(data)\n",
    "\n",
    "        # STEP 9: Start the thread\n",
    "        myHttp = threading.Thread(target=myThread)\n",
    "        myHttp.start()\n",
    "\n",
    "        # STEP 10: Wait for user input to stop recording\n",
    "        input(\"Press Enter to stop recording...\\n\\n\")\n",
    "\n",
    "        # STEP 11: Set the exit flag to True to stop the thread\n",
    "        lock_exit.acquire()\n",
    "        exit = True\n",
    "        lock_exit.release()\n",
    "\n",
    "        # STEP 12: Wait for the thread to finish\n",
    "        myHttp.join()\n",
    "\n",
    "        # STEP 13: Close the connection to Deepgram\n",
    "        dg_connection.finish()\n",
    "\n",
    "        print(\"Finished\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not open socket: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
